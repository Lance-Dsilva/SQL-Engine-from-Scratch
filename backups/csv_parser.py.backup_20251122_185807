"""
Custom CSV Parser without using csv library
Supports streaming for large files (up to 500MB)
"""

class CSVParser:
    def __init__(self, delimiter=',', quote_char='"'):
        self.delimiter = delimiter
        self.quote_char = quote_char
    
    def parse_file(self, filepath):
        """
        Parse CSV file normally (full load) - for files < 100MB
        
        Args:
            filepath: File path or file-like object
            
        Returns:
            dict: {'headers': [...], 'data': [...]}
        """
        # Handle both file paths and file-like objects (Streamlit uploaded files)
        if hasattr(filepath, 'read'):
            content = filepath.read()
            if isinstance(content, bytes):
                content = content.decode('utf-8')
            lines = content.strip().split('\n')
        else:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        
        if not lines:
            return {'headers': [], 'data': []}
        
        # Parse headers
        headers = self._parse_line(lines[0])
        
        # Parse data
        data = []
        for line in lines[1:]:
            if line.strip():
                row_data = self._parse_line(line)
                # Convert to dict
                row_dict = {headers[i]: row_data[i] if i < len(row_data) else None 
                           for i in range(len(headers))}
                data.append(row_dict)
        
        return {
            'headers': headers,
            'data': data
        }
    
    def get_file_info(self, filepath):
        """
        Get file headers and estimate row count without loading entire file
        
        Args:
            filepath (str): Path to CSV file
            
        Returns:
            tuple: (headers, estimated_rows)
        """
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            # Read first line for headers
            first_line = f.readline()
            headers = self._parse_line(first_line)
            
            # Estimate rows by counting lines in first 1MB
            f.seek(0)
            sample = f.read(1024 * 1024)  # Read 1MB
            sample_lines = sample.count('\n')
            
            # Get file size
            import os
            file_size = os.path.getsize(filepath)
            
            # Estimate total lines
            estimated_rows = int((file_size / (1024 * 1024)) * sample_lines)
            
        return headers, estimated_rows
    
    def parse_file_in_chunks(self, filepath, chunk_size=10000):
        """
        Generator that yields chunks of data from large files
        TRUE STREAMING - Does not load entire file into memory
        
        Args:
            filepath (str): Path to CSV file
            chunk_size (int): Number of rows per chunk
            
        Yields:
            list: Chunk of rows (dicts)
        """
        with open(filepath, 'r', encoding='utf-8', errors='ignore', buffering=8192*1024) as f:
            # Read headers
            header_line = f.readline()
            headers = self._parse_line(header_line)
            
            chunk = []
            
            for line in f:
                if not line.strip():
                    continue
                
                try:
                    row_data = self._parse_line(line)
                    # Convert to dict
                    row_dict = {headers[i]: row_data[i] if i < len(row_data) else None 
                               for i in range(len(headers))}
                    chunk.append(row_dict)
                    
                    # Yield chunk when it reaches chunk_size
                    if len(chunk) >= chunk_size:
                        yield chunk
                        chunk = []  # Reset chunk
                        
                except Exception as e:
                    # Skip malformed rows
                    continue
            
            # Yield remaining rows
            if chunk:
                yield chunk
    
    def parse_file_chunked(self, filepath, chunk_size=10000):
        """
        Parse CSV file with chunking support (compatibility method)
        
        Args:
            filepath: File path
            chunk_size (int): Number of rows per chunk
            
        Returns:
            dict: {'headers': [...], 'data': [...], 'estimated_rows': int}
        """
        headers, estimated_rows = self.get_file_info(filepath)
        
        # For compatibility, load all data (but use chunked reading)
        all_data = []
        for chunk in self.parse_file_in_chunks(filepath, chunk_size):
            all_data.extend(chunk)
        
        return {
            'headers': headers,
            'data': all_data,
            'estimated_rows': estimated_rows
        }
    
    def _parse_line(self, line):
        """
        Parse a single CSV line handling quotes and delimiters
        Optimized for performance
        
        Args:
            line (str): Line to parse
            
        Returns:
            list: Parsed values
        """
        line = line.strip()
        values = []
        current_value = ""
        in_quotes = False
        
        i = 0
        while i < len(line):
            char = line[i]
            
            if char == self.quote_char:
                if in_quotes and i + 1 < len(line) and line[i + 1] == self.quote_char:
                    # Escaped quote
                    current_value += self.quote_char
                    i += 1
                else:
                    # Toggle quote mode
                    in_quotes = not in_quotes
            elif char == self.delimiter and not in_quotes:
                # End of value
                values.append(current_value.strip())
                current_value = ""
            else:
                current_value += char
            
            i += 1
        
        # Add last value
        values.append(current_value.strip())
        
        return values
