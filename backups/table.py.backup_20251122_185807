"""
Table class for representing and managing data tables with chunking support
Handles files up to 500MB efficiently
"""

class Table:
    def __init__(self, headers, data, name, chunked=False, chunk_size=None, 
                 file_path=None, use_disk_cache=True, optimize_memory=True):
        """
        Initialize a Table
        
        Args:
            headers (list): List of column names
            data (list): List of rows (None for chunked mode)
            name (str): Table name
            chunked (bool): Whether data is loaded in chunked mode
            chunk_size (int): Size of each chunk if chunked=True
            file_path (str): Path to CSV file for chunked reading
            use_disk_cache (bool): Whether to cache chunks on disk
            optimize_memory (bool): Whether to use aggressive memory optimization
        """
        self.headers = headers
        self.data = data
        self.name = name
        self.chunked = chunked
        self.chunk_size = chunk_size or 10000
        self.file_path = file_path
        self.use_disk_cache = use_disk_cache
        self.optimize_memory = optimize_memory
        self.estimated_rows = 0
        
        # For chunked mode, estimate rows
        if chunked and file_path:
            self._estimate_rows()
        
    def _estimate_rows(self):
        """Estimate number of rows in the file"""
        try:
            import os
            file_size = os.path.getsize(self.file_path)
            # Rough estimate: assuming average 100 bytes per row
            self.estimated_rows = file_size // 100
        except:
            self.estimated_rows = 0
    
    def get_chunks(self):
        """
        Generator to yield chunks of data
        
        Yields:
            list: Chunk of rows
        """
        if not self.chunked:
            yield self.data
        else:
            # For chunked mode, read from file
            from sql_component.parsers.csv_parser import CSVParser
            parser = CSVParser()
            
            for chunk in parser.parse_file_in_chunks(self.file_path, self.chunk_size):
                if self.optimize_memory:
                    # Yield chunk and let Python GC clean up
                    yield chunk
                    # Force garbage collection after each chunk
                    import gc
                    gc.collect()
                else:
                    yield chunk
    
    def __len__(self):
        """Return number of rows"""
        if self.chunked:
            return self.estimated_rows
        return len(self.data) if self.data else 0
    
    def __repr__(self):
        return f"Table(name='{self.name}', rows={len(self)}, columns={len(self.headers)}, chunked={self.chunked})"
