import streamlit as st
from sql_component.parsers.csv_parser import CSVParser
from sql_component.core.table import Table
from sql_component.core.query_engine import QueryEngine

# Page configuration
st.set_page_config(
    page_title="SQL Engine from Scratch",
    page_icon="ğŸ—„ï¸",
    layout="wide"
)

# Initialize session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'loading_mode' not in st.session_state:
    st.session_state.loading_mode = None
if 'table' not in st.session_state:
    st.session_state.table = None
if 'chunk_size' not in st.session_state:
    st.session_state.chunk_size = 10000
if 'query_operations' not in st.session_state:
    st.session_state.query_operations = []
if 'show_code_export' not in st.session_state:
    st.session_state.show_code_export = False
if 'query_result' not in st.session_state:
    st.session_state.query_result = None

# Header
st.title("ğŸ—„ï¸ SQL Engine from Scratch")
st.markdown("Custom CSV processing with optional chunking for large datasets")

# Create tabs - ONLY 2 TABS
tab_names = ["ğŸ“ Load Data", "ğŸ”¨ Build Query"]
tabs = st.tabs(tab_names)

# ==================== TAB 1: LOAD DATA ====================
with tabs[0]:
    st.header("Load Your Data")
    
    st.info("""
    **Choose your loading method:**
    - **Normal Load:** For files that fit in memory (<100MB typically)  
      All data loaded at once for fast operations
    - **Chunked Load:** For large files that exceed memory limits  
      Data processed in batches, use Build Query tab for all operations
    """)
    
    col1, col2 = st.columns(2)
    
    # ===== NORMAL LOAD =====
    with col1:
        st.subheader("ğŸ”· Normal Load")
        st.markdown("""
        Load entire dataset into memory.
        
        âœ… Fast query execution  
        âœ… All operations available  
        âš ï¸ Limited by available RAM
        """)
        
        uploaded_file_normal = st.file_uploader(
            "Upload CSV file", 
            type=['csv'],
            key="normal_upload"
        )
        
        if st.button("Load Data Normally", type="primary", key="load_normal"):
            if uploaded_file_normal is not None:
                try:
                    with st.spinner("Loading data..."):
                        parser = CSVParser()
                        data = parser.parse_file(uploaded_file_normal)
                        
                        st.session_state.table = Table(
                            data['headers'], 
                            data['data'], 
                            uploaded_file_normal.name
                        )
                        st.session_state.data_loaded = True
                        st.session_state.loading_mode = 'normal'
                        st.session_state.query_operations = []
                        st.session_state.query_result = None
                        
                        st.success(f"""
                        âœ… Data loaded successfully!
                        - **File:** {uploaded_file_normal.name}
                        - **Rows:** {len(data['data']):,}
                        - **Columns:** {len(data['headers'])}
                        - **Mode:** Normal (Full load)
                        """)
                        
                        # Show preview
                        st.subheader("Data Preview")
                        preview_data = data['data'][:10]
                        st.dataframe(preview_data, use_container_width=True)
                        
                except Exception as e:
                    st.error(f"âŒ Error loading file: {str(e)}")
            else:
                st.warning("âš ï¸ Please upload a file first")
    
    # ===== CHUNKED LOAD =====
    with col2:
        st.subheader("ğŸ”¶ Chunked Load")
        st.markdown("""
        Process large files in batches.
        
        âœ… Handles files larger than RAM  
        âœ… Memory-efficient processing  
        âš ï¸ Use Build Query tab for operations
        """)
        
        uploaded_file_chunked = st.file_uploader(
            "Upload large CSV file", 
            type=['csv'],
            key="chunked_upload"
        )
        
        chunk_size = st.number_input(
            "Chunk Size (rows per batch)",
            min_value=1000,
            max_value=100000,
            value=10000,
            step=1000,
            help="Number of rows to process in each batch"
        )
        st.session_state.chunk_size = chunk_size
        
        if st.button("Load Data with Chunking", type="primary", key="load_chunked"):
            if uploaded_file_chunked is not None:
                try:
                    with st.spinner(f"Loading data in chunks of {chunk_size:,} rows..."):
                        parser = CSVParser()
                        data = parser.parse_file_chunked(
                            uploaded_file_chunked, 
                            chunk_size=chunk_size
                        )
                        
                        st.session_state.table = Table(
                            data['headers'], 
                            data['data'],
                            uploaded_file_chunked.name,
                            chunked=True,
                            chunk_size=chunk_size
                        )
                        st.session_state.data_loaded = True
                        st.session_state.loading_mode = 'chunked'
                        st.session_state.query_operations = []
                        st.session_state.query_result = None
                        
                        st.success(f"""
                        âœ… Data loaded successfully!
                        - **File:** {uploaded_file_chunked.name}
                        - **Mode:** Chunked ({chunk_size:,} rows/batch)
                        - **Estimated rows:** {data.get('estimated_rows', 'Unknown'):,}
                        - **Columns:** {len(data['headers'])}
                        """)
                        
                        # Show preview
                        st.subheader("Data Preview (First Chunk)")
                        preview_data = data['data'][:10]
                        st.dataframe(preview_data, use_container_width=True)
                        
                except Exception as e:
                    st.error(f"âŒ Error loading file: {str(e)}")
            else:
                st.warning("âš ï¸ Please upload a file first")
    
    # Current status indicator
    if st.session_state.data_loaded:
        st.divider()
        
        mode_icon = "ğŸ”¶" if st.session_state.loading_mode == 'chunked' else "ğŸ”·"
        
        st.success(f"""
        ### ğŸ‰ Data Ready!
        
        **Current Configuration:**
        - {mode_icon} **Mode:** {st.session_state.loading_mode.upper()}
        - ğŸ“Š **Table:** {st.session_state.table.name}
        - ğŸ“ˆ **Rows:** {len(st.session_state.table.data):,}
        - ğŸ“‹ **Columns:** {len(st.session_state.table.headers)}
        {f"- âš™ï¸ **Chunk Size:** {st.session_state.chunk_size:,} rows" if st.session_state.loading_mode == 'chunked' else ""}
        
        â†’ **Navigate to the 'Build Query' tab to process your data**
        """)

# ==================== TAB 2: BUILD QUERY ====================
with tabs[1]:
    st.header("ğŸ”¨ Build Your Query")
    
    # Check if data is loaded
    if not st.session_state.data_loaded:
        st.warning("âš ï¸ Please load data first from the 'Load Data' tab")
        st.stop()
    
    # Mode indicator
    if st.session_state.loading_mode == 'chunked':
        st.info(f"""
        ğŸ”¶ **Chunked Processing Mode** ({st.session_state.chunk_size:,} rows/batch)
        
        Operations will process data in batches to conserve memory. 
        Build your query below by adding operations sequentially.
        """)
    else:
        st.info(f"""
        ğŸ”· **Normal Processing Mode**
        
        All data is loaded in memory for fast operations.
        Build your query below by adding operations sequentially.
        """)
    
    st.markdown("""
    Build complex queries by chaining operations. Each operation is applied in sequence.
    Perfect for chunked processing where operations are applied batch-by-batch.
    """)
    
    # ==================== QUERY VISUALIZATION ====================
    st.subheader("ğŸ“‹ Current Query Chain")
    
    if st.session_state.query_operations:
        # Flow diagram
        query_display = "**ğŸ“Š Data** â†’ "
        for i, op in enumerate(st.session_state.query_operations):
            query_display += f"**{op['type'].upper()}** â†’ "
        query_display += "**âœ¨ Result**"
        
        st.markdown(query_display)
        
        # Operations list with details
        st.markdown("---")
        for i, op in enumerate(st.session_state.query_operations):
            col1, col2 = st.columns([4, 1])
            
            with col1:
                with st.container():
                    st.markdown(f"**Operation {i+1}: {op['type'].upper()}**")
                    
                    if op['type'] == 'filter':
                        st.text(f"   Column: {op['column']}")
                        st.text(f"   Condition: {op['condition']}")
                    
                    elif op['type'] == 'select':
                        st.text(f"   Columns: {', '.join(op['columns'])}")
                    
                    elif op['type'] == 'group_by':
                        st.text(f"   Group By: {op['column']}")
                        if 'agg_func' in op:
                            st.text(f"   Aggregate: {op['agg_func']}({op['agg_column']})")
                    
                    elif op['type'] == 'aggregate':
                        st.text(f"   Function: {op['function']}({op['column']})")
                    
                    elif op['type'] == 'order_by':
                        st.text(f"   Sort By: {op['column']} ({op['order']})")
                    
                    elif op['type'] == 'limit':
                        st.text(f"   Limit: {op['limit']} rows")
                    
                    elif op['type'] == 'join':
                        st.text(f"   Join Type: {op['join_type']}")
                        st.text(f"   On Column: {op['column']}")
            
            with col2:
                if st.button("ğŸ—‘ï¸ Remove", key=f"remove_{i}"):
                    st.session_state.query_operations.pop(i)
                    st.rerun()
        
        # Performance analysis for chunked mode
        if st.session_state.loading_mode == 'chunked':
            st.markdown("---")
            st.subheader("âš¡ Chunked Processing Analysis")
            
            has_expensive_ops = any(op['type'] in ['order_by', 'join'] 
                                   for op in st.session_state.query_operations)
            has_groupby = any(op['type'] == 'group_by' 
                            for op in st.session_state.query_operations)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Operations", len(st.session_state.query_operations))
            with col2:
                memory_impact = "ğŸ”´ High" if has_expensive_ops or has_groupby else "ğŸŸ¢ Low"
                st.metric("Memory Impact", memory_impact)
            with col3:
                efficiency = "ğŸ”´ Poor" if has_expensive_ops else "ğŸŸ¢ Good"
                st.metric("Chunk Efficiency", efficiency)
            
            if has_expensive_ops:
                st.error("""
                âš ï¸ **Warning:** Your query contains operations (ORDER BY or JOIN) that require 
                loading all data into memory, which defeats the purpose of chunked processing.
                Consider removing these operations or using normal load mode.
                """)
            elif has_groupby:
                st.warning("""
                âš ï¸ **Note:** GROUP BY operations accumulate results across chunks. 
                High cardinality (many unique groups) may still use significant memory.
                """)
            else:
                st.success("""
                âœ… **Optimal:** Your query operations work efficiently with chunked processing, 
                maintaining low memory usage throughout execution.
                """)
    else:
        st.info("ğŸ‘† No operations added yet. Add operations below to build your query.")
    
    st.markdown("---")
    
    # ==================== ADD NEW OPERATION ====================
    st.subheader("â• Add Operation")
    
    operation_type = st.selectbox(
        "Select Operation Type",
        ["Filter (WHERE)", "Select (PROJECT)", "Group By", "Aggregate", "Order By", "Limit", "Join"],
        key="new_op_type"
    )
    
    # FILTER
    if operation_type == "Filter (WHERE)":
        col1, col2, col3 = st.columns(3)
        with col1:
            filter_col = st.selectbox("Column", st.session_state.table.headers, key="filter_col")
        with col2:
            filter_op = st.selectbox("Operator", [">", "<", ">=", "<=", "==", "!="], key="filter_op")
        with col3:
            filter_val = st.text_input("Value", key="filter_val", placeholder="100")
        
        st.caption("ğŸ’¡ Filter works efficiently in chunked mode - applied per batch")
        
        if st.button("â• Add Filter", type="primary", use_container_width=True):
            if filter_val:
                st.session_state.query_operations.append({
                    'type': 'filter',
                    'column': filter_col,
                    'operator': filter_op,
                    'value': filter_val,
                    'condition': f"{filter_col} {filter_op} {filter_val}"
                })
                st.rerun()
            else:
                st.warning("Please enter a value")
    
    # SELECT
    elif operation_type == "Select (PROJECT)":
        selected_cols = st.multiselect(
            "Select Columns to Keep",
            st.session_state.table.headers,
            key="select_cols"
        )
        
        st.caption("ğŸ’¡ Projection works efficiently in chunked mode - applied per batch")
        
        if st.button("â• Add Select", type="primary", use_container_width=True):
            if selected_cols:
                st.session_state.query_operations.append({
                    'type': 'select',
                    'columns': selected_cols
                })
                st.rerun()
            else:
                st.warning("Please select at least one column")
    
    # GROUP BY
    elif operation_type == "Group By":
        col1, col2 = st.columns(2)
        with col1:
            group_col = st.selectbox("Group By Column", st.session_state.table.headers, key="group_col")
        with col2:
            include_agg = st.checkbox("Include Aggregation", key="include_agg")
        
        if include_agg:
            col1, col2 = st.columns(2)
            with col1:
                agg_func = st.selectbox("Function", ["SUM", "AVG", "COUNT", "MIN", "MAX"], key="agg_func_gb")
            with col2:
                agg_col = st.selectbox("Column", st.session_state.table.headers, key="agg_col_gb")
            
            st.caption("âš ï¸ GROUP BY accumulates results across chunks - may use more memory")
            
            if st.button("â• Add Group By + Aggregate", type="primary", use_container_width=True):
                st.session_state.query_operations.append({
                    'type': 'group_by',
                    'column': group_col,
                    'agg_func': agg_func,
                    'agg_column': agg_col
                })
                st.rerun()
        else:
            st.caption("âš ï¸ GROUP BY accumulates results across chunks - may use more memory")
            
            if st.button("â• Add Group By", type="primary", use_container_width=True):
                st.session_state.query_operations.append({
                    'type': 'group_by',
                    'column': group_col
                })
                st.rerun()
    
    # AGGREGATE
    elif operation_type == "Aggregate":
        col1, col2 = st.columns(2)
        with col1:
            agg_func = st.selectbox("Function", ["SUM", "AVG", "COUNT", "MIN", "MAX"], key="agg_only_func")
        with col2:
            agg_col = st.selectbox("Column", st.session_state.table.headers, key="agg_only_col")
        
        st.caption("ğŸ’¡ Aggregation works well in chunked mode - combines partial results")
        
        if st.button("â• Add Aggregate", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'aggregate',
                'function': agg_func,
                'column': agg_col
            })
            st.rerun()
    
    # ORDER BY
    elif operation_type == "Order By":
        col1, col2 = st.columns(2)
        with col1:
            sort_col = st.selectbox("Sort Column", st.session_state.table.headers, key="sort_col")
        with col2:
            sort_order = st.selectbox("Order", ["Ascending", "Descending"], key="sort_order")
        
        if st.session_state.loading_mode == 'chunked':
            st.caption("âš ï¸ ORDER BY requires loading all data - defeats chunked processing")
        else:
            st.caption("ğŸ’¡ ORDER BY works efficiently in normal mode")
        
        if st.button("â• Add Order By", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'order_by',
                'column': sort_col,
                'order': sort_order
            })
            st.rerun()
    
    # LIMIT
    elif operation_type == "Limit":
        limit_val = st.number_input("Number of Rows", min_value=1, value=10, step=1, key="limit_val")
        
        st.caption("ğŸ’¡ Limit works efficiently in both modes")
        
        if st.button("â• Add Limit", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'limit',
                'limit': limit_val
            })
            st.rerun()
    
    # JOIN
    elif operation_type == "Join":
        st.warning("âš ï¸ Join functionality requires uploading a second table")
        
        join_file = st.file_uploader("Upload second table", type=['csv'], key="join_file")
        
        if join_file:
            col1, col2, col3 = st.columns(3)
            with col1:
                join_type = st.selectbox("Join Type", ["INNER", "LEFT", "RIGHT", "OUTER"], key="join_type")
            with col2:
                join_col = st.selectbox("Join Column", st.session_state.table.headers, key="join_col")
            with col3:
                join_on_col = st.text_input("Join ON Column (in 2nd table)", key="join_on_col")
            
            if st.session_state.loading_mode == 'chunked':
                st.caption("âš ï¸ JOIN requires loading tables - defeats chunked processing")
            else:
                st.caption("ğŸ’¡ JOIN works efficiently in normal mode")
            
            if st.button("â• Add Join", type="primary", use_container_width=True):
                st.session_state.query_operations.append({
                    'type': 'join',
                    'join_type': join_type,
                    'column': join_col,
                    'join_on': join_on_col,
                    'join_file': join_file.name
                })
                st.rerun()
    
    st.markdown("---")
    
    # ==================== EXECUTE QUERY ====================
    if st.session_state.query_operations:
        st.subheader("ğŸš€ Execute Query")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸš€ Execute Query", type="primary", use_container_width=True):
                with st.spinner("Executing query..."):
                    try:
                        query = QueryEngine(st.session_state.table)
                        
                        # Apply each operation
                        for op in st.session_state.query_operations:
                            if op['type'] == 'filter':
                                col = op['column']
                                operator = op['operator']
                                value = op['value']
                                try:
                                    value = float(value)
                                except:
                                    pass
                                query = query.filter(lambda row: eval(f"row['{col}'] {operator} {repr(value)}"))
                            
                            elif op['type'] == 'select':
                                query = query.select(op['columns'])
                            
                            elif op['type'] == 'group_by':
                                query = query.group_by(op['column'])
                                if 'agg_func' in op:
                                    query = query.aggregate(op['agg_column'], op['agg_func'])
                            
                            elif op['type'] == 'aggregate':
                                query = query.aggregate(op['column'], op['function'])
                            
                            elif op['type'] == 'order_by':
                                query = query.order_by(op['column'], 
                                                      ascending=(op['order'] == "Ascending"))
                            
                            elif op['type'] == 'limit':
                                query = query.limit(op['limit'])
                        
                        result = query.execute()
                        st.session_state.query_result = result
                        
                        st.success(f"âœ… Query executed successfully! Returned {len(result):,} rows")
                        
                    except Exception as e:
                        st.error(f"âŒ Error executing query: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
        
        with col2:
            if st.button("ğŸ§¹ Clear All Operations", use_container_width=True):
                st.session_state.query_operations = []
                st.session_state.query_result = None
                st.rerun()
        
        with col3:
            if st.button("ğŸ“„ Export Code", use_container_width=True):
                st.session_state.show_code_export = not st.session_state.get('show_code_export', False)
                st.rerun()
        
        # Show generated code
        if st.session_state.get('show_code_export', False):
            st.markdown("---")
            st.subheader("ğŸ“ Generated Python Code")
            
            code = "from sql_component.core.query_engine import QueryEngine\n\n"
            code += "# Build and execute query\n"
            code += "result = (QueryEngine(table)\n"
            
            for op in st.session_state.query_operations:
                if op['type'] == 'filter':
                    code += f"    .filter(lambda row: row['{op['column']}'] {op['operator']} {op['value']})\n"
                elif op['type'] == 'select':
                    code += f"    .select({op['columns']})\n"
                elif op['type'] == 'group_by':
                    code += f"    .group_by('{op['column']}')\n"
                    if 'agg_func' in op:
                        code += f"    .aggregate('{op['agg_column']}', '{op['agg_func']}')\n"
                elif op['type'] == 'aggregate':
                    code += f"    .aggregate('{op['column']}', '{op['function']}')\n"
                elif op['type'] == 'order_by':
                    asc = "True" if op['order'] == "Ascending" else "False"
                    code += f"    .order_by('{op['column']}', ascending={asc})\n"
                elif op['type'] == 'limit':
                    code += f"    .limit({op['limit']})\n"
            
            code += "    .execute())\n"
            
            st.code(code, language="python")
    
    # ==================== SHOW RESULTS ====================
    if st.session_state.query_result is not None:
        st.markdown("---")
        st.subheader("ğŸ“Š Query Results")
        
        result = st.session_state.query_result
        
        if isinstance(result, list) and len(result) > 0:
            st.dataframe(result, use_container_width=True, height=400)
            
            # Download button
            try:
                import pandas as pd
                df = pd.DataFrame(result)
                csv = df.to_csv(index=False)
                st.download_button(
                    label="â¬‡ï¸ Download Results as CSV",
                    data=csv,
                    file_name="query_results.csv",
                    mime="text/csv"
                )
            except:
                pass
        elif isinstance(result, dict):
            st.json(result)
        else:
            st.info("Query executed but returned no data or aggregate result")
            st.write(result)

# ==================== SIDEBAR ====================
with st.sidebar:
    st.header("â„¹ï¸ About")
    st.markdown("""
    **SQL Engine from Scratch**
    
    Custom CSV processing without pandas or csv library.
    
    **Features:**
    - âœ… Custom CSV parser
    - âœ… Normal and chunked loading
    - âœ… Query builder interface
    - âœ… Memory-efficient operations
    - âœ… Code export
    """)
    
    if st.session_state.data_loaded:
        st.divider()
        st.subheader("ğŸ“Š Current Session")
        
        mode_emoji = "ğŸ”¶" if st.session_state.loading_mode == 'chunked' else "ğŸ”·"
        st.markdown(f"**Mode:** {mode_emoji} {st.session_state.loading_mode.upper()}")
        st.markdown(f"**Table:** {st.session_state.table.name}")
        st.markdown(f"**Rows:** {len(st.session_state.table.data):,}")
        st.markdown(f"**Columns:** {len(st.session_state.table.headers)}")
        if st.session_state.loading_mode == 'chunked':
            st.markdown(f"**Chunk Size:** {st.session_state.chunk_size:,}")
        
        st.divider()
        st.markdown(f"**Operations:** {len(st.session_state.query_operations)}")
    
    st.divider()
    st.caption("Built by: Lance Dsilva, Rafayel Mirijanyan, Chelroy Limas")
    st.caption("DSCI 551 - Fall 2025")
