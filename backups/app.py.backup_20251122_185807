import streamlit as st
import tempfile
import os
from sql_component.parsers.csv_parser import CSVParser
from sql_component.core.table import Table
from sql_component.core.query_engine import QueryEngine

# Page configuration
st.set_page_config(
    page_title="SQL Engine from Scratch",
    page_icon="üóÑÔ∏è",
    layout="wide"
)

# Initialize session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'loading_mode' not in st.session_state:
    st.session_state.loading_mode = None
if 'table' not in st.session_state:
    st.session_state.table = None
if 'chunk_size' not in st.session_state:
    st.session_state.chunk_size = 10000
if 'query_operations' not in st.session_state:
    st.session_state.query_operations = []
if 'show_code_export' not in st.session_state:
    st.session_state.show_code_export = False
if 'query_result' not in st.session_state:
    st.session_state.query_result = None
if 'temp_file_path' not in st.session_state:
    st.session_state.temp_file_path = None

# Header
st.title("üóÑÔ∏è SQL Engine from Scratch")
st.markdown("Custom CSV processing with chunking support for files up to 500MB")

# Create tabs - ONLY 2 TABS
tab_names = ["üìÅ Load Data", "üî® Build Query"]
tabs = st.tabs(tab_names)

def get_file_size_mb(file):
    """Get file size in MB"""
    if hasattr(file, 'size'):
        return file.size / (1024 * 1024)
    return 0

# ==================== TAB 1: LOAD DATA ====================
with tabs[0]:
    st.header("Load Your Data")
    
    st.info("""
    **Choose your loading method:**
    - **Normal Load:** For files up to 100MB - Fast, all in memory
    - **Chunked Load:** For files 100MB-500MB - Memory-efficient batch processing
    
    **Maximum supported file size: 500MB**
    """)
    
    col1, col2 = st.columns(2)
    
    # ===== NORMAL LOAD =====
    with col1:
        st.subheader("üî∑ Normal Load")
        st.markdown("""
        **Recommended for files < 100MB**
        
        ‚úÖ Fast query execution  
        ‚úÖ All operations available  
        ‚úÖ Optimal for small to medium files
        ‚ö†Ô∏è May fail on large files (>100MB)
        """)
        
        uploaded_file_normal = st.file_uploader(
            "Upload CSV file (Max 100MB recommended)", 
            type=['csv'],
            key="normal_upload"
        )
        
        if uploaded_file_normal:
            file_size = get_file_size_mb(uploaded_file_normal)
            st.caption(f"üìä File size: {file_size:.2f} MB")
            
            if file_size > 100:
                st.warning(f"‚ö†Ô∏è File is {file_size:.2f}MB. Consider using Chunked Load for better performance.")
        
        if st.button("Load Data Normally", type="primary", key="load_normal"):
            if uploaded_file_normal is not None:
                file_size = get_file_size_mb(uploaded_file_normal)
                
                if file_size > 500:
                    st.error("‚ùå File too large! Maximum 500MB supported. Please use a smaller file.")
                else:
                    try:
                        with st.spinner("Loading data..."):
                            parser = CSVParser()
                            data = parser.parse_file(uploaded_file_normal)
                            
                            st.session_state.table = Table(
                                data['headers'], 
                                data['data'], 
                                uploaded_file_normal.name
                            )
                            st.session_state.data_loaded = True
                            st.session_state.loading_mode = 'normal'
                            st.session_state.query_operations = []
                            st.session_state.query_result = None
                            
                            st.success(f"""
                            ‚úÖ Data loaded successfully!
                            - **File:** {uploaded_file_normal.name}
                            - **Size:** {file_size:.2f} MB
                            - **Rows:** {len(data['data']):,}
                            - **Columns:** {len(data['headers'])}
                            - **Mode:** Normal (Full load)
                            """)
                            
                            # Show preview
                            st.subheader("Data Preview")
                            preview_data = data['data'][:10]
                            st.dataframe(preview_data, use_container_width=True)
                            
                    except MemoryError:
                        st.error("‚ùå Out of memory! File too large for normal load. Please use Chunked Load.")
                    except Exception as e:
                        st.error(f"‚ùå Error loading file: {str(e)}")
            else:
                st.warning("‚ö†Ô∏è Please upload a file first")
    
    # ===== CHUNKED LOAD =====
    with col2:
        st.subheader("üî∂ Chunked Load")
        st.markdown("""
        **Recommended for files 100MB-500MB**
        
        ‚úÖ Handles large files (up to 500MB)  
        ‚úÖ Memory-efficient processing  
        ‚úÖ Prevents out-of-memory errors  
        ‚ö†Ô∏è Uses disk for temporary storage
        """)
        
        uploaded_file_chunked = st.file_uploader(
            "Upload large CSV file (100MB-500MB)", 
            type=['csv'],
            key="chunked_upload"
        )
        
        if uploaded_file_chunked:
            file_size = get_file_size_mb(uploaded_file_chunked)
            st.caption(f"üìä File size: {file_size:.2f} MB")
            
            if file_size > 500:
                st.error("‚ùå File exceeds 500MB limit!")
            elif file_size < 100:
                st.info("‚ÑπÔ∏è File is small enough for Normal Load, but Chunked Load will work too.")
        
        chunk_size = st.number_input(
            "Chunk Size (rows per batch)",
            min_value=1000,
            max_value=50000,
            value=10000,
            step=1000,
            help="Larger chunks = faster but more memory. Start with 10,000."
        )
        st.session_state.chunk_size = chunk_size
        
        # Advanced options
        with st.expander("‚öôÔ∏è Advanced Options"):
            use_disk_cache = st.checkbox("Use disk caching", value=True, 
                help="Store chunks temporarily on disk to save memory")
            optimize_memory = st.checkbox("Optimize memory usage", value=True,
                help="Aggressive memory management for very large files")
        
        if st.button("Load Data with Chunking", type="primary", key="load_chunked"):
            if uploaded_file_chunked is not None:
                file_size = get_file_size_mb(uploaded_file_chunked)
                
                if file_size > 500:
                    st.error("‚ùå File too large! Maximum 500MB supported.")
                else:
                    try:
                        with st.spinner(f"Loading {file_size:.2f}MB file in chunks of {chunk_size:,} rows..."):
                            # Save uploaded file to temporary location for chunked reading
                            with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.csv') as tmp:
                                tmp.write(uploaded_file_chunked.getvalue())
                                temp_path = tmp.name
                            
                            st.session_state.temp_file_path = temp_path
                            
                            parser = CSVParser()
                            
                            # Get headers and estimate rows
                            headers, estimated_rows = parser.get_file_info(temp_path)
                            
                            st.session_state.table = Table(
                                headers=headers,
                                data=None,  # Data not loaded yet
                                name=uploaded_file_chunked.name,
                                chunked=True,
                                chunk_size=chunk_size,
                                file_path=temp_path,
                                use_disk_cache=use_disk_cache,
                                optimize_memory=optimize_memory
                            )
                            st.session_state.data_loaded = True
                            st.session_state.loading_mode = 'chunked'
                            st.session_state.query_operations = []
                            st.session_state.query_result = None
                            
                            st.success(f"""
                            ‚úÖ Data loaded successfully!
                            - **File:** {uploaded_file_chunked.name}
                            - **Size:** {file_size:.2f} MB
                            - **Mode:** Chunked ({chunk_size:,} rows/batch)
                            - **Estimated rows:** ~{estimated_rows:,}
                            - **Columns:** {len(headers)}
                            - **Disk caching:** {'Enabled' if use_disk_cache else 'Disabled'}
                            """)
                            
                            # Show preview
                            st.subheader("Data Preview (First 10 rows)")
                            preview_data = []
                            for i, chunk in enumerate(parser.parse_file_in_chunks(temp_path, chunk_size)):
                                preview_data.extend(chunk[:10])
                                break
                            st.dataframe(preview_data, use_container_width=True)
                            
                    except MemoryError:
                        st.error("‚ùå Out of memory! Try reducing chunk size or enabling disk caching.")
                    except Exception as e:
                        st.error(f"‚ùå Error loading file: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
            else:
                st.warning("‚ö†Ô∏è Please upload a file first")
    
    # Current status indicator
    if st.session_state.data_loaded:
        st.divider()
        
        mode_icon = "üî∂" if st.session_state.loading_mode == 'chunked' else "üî∑"
        
        st.success(f"""
        ### üéâ Data Ready!
        
        **Current Configuration:**
        - {mode_icon} **Mode:** {st.session_state.loading_mode.upper()}
        - üìä **Table:** {st.session_state.table.name}
        {f"- üìà **Estimated Rows:** ~{st.session_state.table.estimated_rows:,}" if st.session_state.loading_mode == 'chunked' else f"- üìà **Rows:** {len(st.session_state.table.data):,}"}
        - üìã **Columns:** {len(st.session_state.table.headers)}
        {f"- ‚öôÔ∏è **Chunk Size:** {st.session_state.chunk_size:,} rows" if st.session_state.loading_mode == 'chunked' else ""}
        
        ‚Üí **Navigate to the 'Build Query' tab to process your data**
        """)

# ==================== TAB 2: BUILD QUERY ====================
with tabs[1]:
    st.header("üî® Build Your Query")
    
    # Check if data is loaded
    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Please load data first from the 'Load Data' tab")
        st.stop()
    
    # Mode indicator
    if st.session_state.loading_mode == 'chunked':
        st.info(f"""
        üî∂ **Chunked Processing Mode** ({st.session_state.chunk_size:,} rows/batch)
        
        Operations will process data in batches to conserve memory. 
        Perfect for large files (100MB-500MB).
        """)
    else:
        st.info(f"""
        üî∑ **Normal Processing Mode**
        
        All data is loaded in memory for fast operations.
        """)
    
    st.markdown("""
    Build complex queries by chaining operations. Each operation is applied in sequence.
    """)
    
    # ==================== QUERY VISUALIZATION ====================
    st.subheader("üìã Current Query Chain")
    
    if st.session_state.query_operations:
        # Flow diagram
        query_display = "**üìä Data** ‚Üí "
        for i, op in enumerate(st.session_state.query_operations):
            query_display += f"**{op['type'].upper()}** ‚Üí "
        query_display += "**‚ú® Result**"
        
        st.markdown(query_display)
        
        # Operations list with details
        st.markdown("---")
        for i, op in enumerate(st.session_state.query_operations):
            col1, col2 = st.columns([4, 1])
            
            with col1:
                with st.container():
                    st.markdown(f"**Operation {i+1}: {op['type'].upper()}**")
                    
                    if op['type'] == 'filter':
                        st.text(f"   Column: {op['column']}")
                        st.text(f"   Condition: {op['condition']}")
                    
                    elif op['type'] == 'select':
                        st.text(f"   Columns: {', '.join(op['columns'])}")
                    
                    elif op['type'] == 'group_by':
                        st.text(f"   Group By: {op['column']}")
                        if 'agg_func' in op:
                            st.text(f"   Aggregate: {op['agg_func']}({op['agg_column']})")
                    
                    elif op['type'] == 'aggregate':
                        st.text(f"   Function: {op['function']}({op['column']})")
                    
                    elif op['type'] == 'order_by':
                        st.text(f"   Sort By: {op['column']} ({op['order']})")
                    
                    elif op['type'] == 'limit':
                        st.text(f"   Limit: {op['limit']} rows")
            
            with col2:
                if st.button("üóëÔ∏è Remove", key=f"remove_{i}"):
                    st.session_state.query_operations.pop(i)
                    st.rerun()
        
        # Performance analysis for chunked mode
        if st.session_state.loading_mode == 'chunked':
            st.markdown("---")
            st.subheader("‚ö° Chunked Processing Analysis")
            
            has_expensive_ops = any(op['type'] in ['order_by', 'join'] 
                                   for op in st.session_state.query_operations)
            has_groupby = any(op['type'] == 'group_by' 
                            for op in st.session_state.query_operations)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Operations", len(st.session_state.query_operations))
            with col2:
                memory_impact = "üî¥ High" if has_expensive_ops or has_groupby else "üü¢ Low"
                st.metric("Memory Impact", memory_impact)
            with col3:
                efficiency = "üî¥ Poor" if has_expensive_ops else "üü¢ Excellent"
                st.metric("Chunk Efficiency", efficiency)
            
            if has_expensive_ops:
                st.error("""
                ‚ö†Ô∏è **Warning:** ORDER BY/JOIN operations require loading all data into memory.
                For files >200MB, this may cause memory issues. Consider:
                - Filtering data first to reduce size
                - Using LIMIT before sorting
                - Processing on a machine with more RAM
                """)
            elif has_groupby:
                st.warning("""
                ‚ö†Ô∏è **Note:** GROUP BY accumulates results across chunks. 
                For large files with high cardinality (many unique groups), memory usage may increase.
                """)
            else:
                st.success("""
                ‚úÖ **Optimal:** Your query operations work efficiently with chunked processing!
                Filter and Select operations process each chunk independently with minimal memory.
                """)
    else:
        st.info("üëÜ No operations added yet. Add operations below to build your query.")
    
    st.markdown("---")
    
    # ==================== ADD NEW OPERATION ====================
    st.subheader("‚ûï Add Operation")
    
    operation_type = st.selectbox(
        "Select Operation Type",
        ["Filter (WHERE)", "Select (PROJECT)", "Group By", "Aggregate", "Order By", "Limit"],
        key="new_op_type"
    )
    
    # FILTER
    if operation_type == "Filter (WHERE)":
        col1, col2, col3 = st.columns(3)
        with col1:
            filter_col = st.selectbox("Column", st.session_state.table.headers, key="filter_col")
        with col2:
            filter_op = st.selectbox("Operator", [">", "<", ">=", "<=", "==", "!=", "contains"], key="filter_op")
        with col3:
            filter_val = st.text_input("Value", key="filter_val", placeholder="100")
        
        st.caption("üí° ‚úÖ EXCELLENT for chunked mode - Applied per batch, minimal memory")
        
        if st.button("‚ûï Add Filter", type="primary", use_container_width=True):
            if filter_val:
                st.session_state.query_operations.append({
                    'type': 'filter',
                    'column': filter_col,
                    'operator': filter_op,
                    'value': filter_val,
                    'condition': f"{filter_col} {filter_op} {filter_val}"
                })
                st.rerun()
            else:
                st.warning("Please enter a value")
    
    # SELECT
    elif operation_type == "Select (PROJECT)":
        selected_cols = st.multiselect(
            "Select Columns to Keep",
            st.session_state.table.headers,
            key="select_cols"
        )
        
        st.caption("üí° ‚úÖ EXCELLENT for chunked mode - Applied per batch, reduces memory")
        
        if st.button("‚ûï Add Select", type="primary", use_container_width=True):
            if selected_cols:
                st.session_state.query_operations.append({
                    'type': 'select',
                    'columns': selected_cols
                })
                st.rerun()
            else:
                st.warning("Please select at least one column")
    
    # GROUP BY
    elif operation_type == "Group By":
        col1, col2 = st.columns(2)
        with col1:
            group_col = st.selectbox("Group By Column", st.session_state.table.headers, key="group_col")
        with col2:
            include_agg = st.checkbox("Include Aggregation", key="include_agg")
        
        if include_agg:
            col1, col2 = st.columns(2)
            with col1:
                agg_func = st.selectbox("Function", ["SUM", "AVG", "COUNT", "MIN", "MAX"], key="agg_func_gb")
            with col2:
                agg_col = st.selectbox("Column", st.session_state.table.headers, key="agg_col_gb")
            
            st.caption("‚ö†Ô∏è MODERATE for chunked mode - Accumulates groups, memory depends on cardinality")
            
            if st.button("‚ûï Add Group By + Aggregate", type="primary", use_container_width=True):
                st.session_state.query_operations.append({
                    'type': 'group_by',
                    'column': group_col,
                    'agg_func': agg_func,
                    'agg_column': agg_col
                })
                st.rerun()
        else:
            st.caption("‚ö†Ô∏è MODERATE for chunked mode - Accumulates groups, memory depends on cardinality")
            
            if st.button("‚ûï Add Group By", type="primary", use_container_width=True):
                st.session_state.query_operations.append({
                    'type': 'group_by',
                    'column': group_col
                })
                st.rerun()
    
    # AGGREGATE
    elif operation_type == "Aggregate":
        col1, col2 = st.columns(2)
        with col1:
            agg_func = st.selectbox("Function", ["SUM", "AVG", "COUNT", "MIN", "MAX"], key="agg_only_func")
        with col2:
            agg_col = st.selectbox("Column", st.session_state.table.headers, key="agg_only_col")
        
        st.caption("üí° ‚úÖ GOOD for chunked mode - Combines partial results efficiently")
        
        if st.button("‚ûï Add Aggregate", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'aggregate',
                'function': agg_func,
                'column': agg_col
            })
            st.rerun()
    
    # ORDER BY
    elif operation_type == "Order By":
        col1, col2 = st.columns(2)
        with col1:
            sort_col = st.selectbox("Sort Column", st.session_state.table.headers, key="sort_col")
        with col2:
            sort_order = st.selectbox("Order", ["Ascending", "Descending"], key="sort_order")
        
        if st.session_state.loading_mode == 'chunked':
            st.caption("üî¥ POOR for chunked mode - Requires loading ALL data into memory")
        else:
            st.caption("üí° ‚úÖ GOOD for normal mode")
        
        if st.button("‚ûï Add Order By", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'order_by',
                'column': sort_col,
                'order': sort_order
            })
            st.rerun()
    
    # LIMIT
    elif operation_type == "Limit":
        limit_val = st.number_input("Number of Rows", min_value=1, value=100, step=10, key="limit_val")
        
        st.caption("üí° ‚úÖ EXCELLENT for chunked mode - Stops processing early, saves time and memory")
        
        if st.button("‚ûï Add Limit", type="primary", use_container_width=True):
            st.session_state.query_operations.append({
                'type': 'limit',
                'limit': limit_val
            })
            st.rerun()
    
    st.markdown("---")
    
    # ==================== EXECUTE QUERY ====================
    if st.session_state.query_operations:
        st.subheader("üöÄ Execute Query")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üöÄ Execute Query", type="primary", use_container_width=True):
                with st.spinner("Executing query..."):
                    try:
                        query = QueryEngine(st.session_state.table)
                        
                        # Apply each operation
                        for op in st.session_state.query_operations:
                            if op['type'] == 'filter':
                                col = op['column']
                                operator = op['operator']
                                value = op['value']
                                
                                # Handle different operators
                                if operator == 'contains':
                                    query = query.filter(lambda row: value.lower() in str(row.get(col, '')).lower())
                                else:
                                    try:
                                        value = float(value)
                                    except:
                                        pass
                                    query = query.filter(lambda row: eval(f"row.get('{col}') {operator} {repr(value)}"))
                            
                            elif op['type'] == 'select':
                                query = query.select(op['columns'])
                            
                            elif op['type'] == 'group_by':
                                query = query.group_by(op['column'])
                                if 'agg_func' in op:
                                    query = query.aggregate(op['agg_column'], op['agg_func'])
                            
                            elif op['type'] == 'aggregate':
                                query = query.aggregate(op['column'], op['function'])
                            
                            elif op['type'] == 'order_by':
                                query = query.order_by(op['column'], 
                                                      ascending=(op['order'] == "Ascending"))
                            
                            elif op['type'] == 'limit':
                                query = query.limit(op['limit'])
                        
                        result = query.execute()
                        st.session_state.query_result = result
                        
                        result_size = len(result) if isinstance(result, list) else 1
                        st.success(f"‚úÖ Query executed successfully! Returned {result_size:,} rows")
                        
                    except Exception as e:
                        st.error(f"‚ùå Error executing query: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
        
        with col2:
            if st.button("üßπ Clear All Operations", use_container_width=True):
                st.session_state.query_operations = []
                st.session_state.query_result = None
                st.rerun()
        
        with col3:
            if st.button("üìÑ Export Code", use_container_width=True):
                st.session_state.show_code_export = not st.session_state.get('show_code_export', False)
                st.rerun()
        
        # Show generated code
        if st.session_state.get('show_code_export', False):
            st.markdown("---")
            st.subheader("üìù Generated Python Code")
            
            code = "from sql_component.core.query_engine import QueryEngine\n\n"
            code += "# Build and execute query\n"
            code += "result = (QueryEngine(table)\n"
            
            for op in st.session_state.query_operations:
                if op['type'] == 'filter':
                    if op['operator'] == 'contains':
                        code += f"    .filter(lambda row: '{op['value']}'.lower() in str(row.get('{op['column']}', '')).lower())\n"
                    else:
                        code += f"    .filter(lambda row: row.get('{op['column']}') {op['operator']} {op['value']})\n"
                elif op['type'] == 'select':
                    code += f"    .select({op['columns']})\n"
                elif op['type'] == 'group_by':
                    code += f"    .group_by('{op['column']}')\n"
                    if 'agg_func' in op:
                        code += f"    .aggregate('{op['agg_column']}', '{op['agg_func']}')\n"
                elif op['type'] == 'aggregate':
                    code += f"    .aggregate('{op['column']}', '{op['function']}')\n"
                elif op['type'] == 'order_by':
                    asc = "True" if op['order'] == "Ascending" else "False"
                    code += f"    .order_by('{op['column']}', ascending={asc})\n"
                elif op['type'] == 'limit':
                    code += f"    .limit({op['limit']})\n"
            
            code += "    .execute())\n"
            
            st.code(code, language="python")
    
    # ==================== SHOW RESULTS ====================
    if st.session_state.query_result is not None:
        st.markdown("---")
        st.subheader("üìä Query Results")
        
        result = st.session_state.query_result
        
        if isinstance(result, list) and len(result) > 0:
            st.dataframe(result, use_container_width=True, height=400)
            
            # Download button
            try:
                import pandas as pd
                df = pd.DataFrame(result)
                csv = df.to_csv(index=False)
                st.download_button(
                    label="‚¨áÔ∏è Download Results as CSV",
                    data=csv,
                    file_name="query_results.csv",
                    mime="text/csv"
                )
            except:
                pass
        elif isinstance(result, dict):
            st.json(result)
        else:
            st.info("Query executed")
            st.write(result)

# ==================== SIDEBAR ====================
with st.sidebar:
    st.header("‚ÑπÔ∏è About")
    st.markdown("""
    **SQL Engine from Scratch**
    
    Supports files up to **500MB** with chunked processing!
    
    **Features:**
    - ‚úÖ Custom CSV parser
    - ‚úÖ Normal load (< 100MB)
    - ‚úÖ Chunked load (100-500MB)
    - ‚úÖ Query builder interface
    - ‚úÖ Memory-efficient operations
    - ‚úÖ Disk caching option
    """)
    
    if st.session_state.data_loaded:
        st.divider()
        st.subheader("üìä Current Session")
        
        mode_emoji = "üî∂" if st.session_state.loading_mode == 'chunked' else "üî∑"
        st.markdown(f"**Mode:** {mode_emoji} {st.session_state.loading_mode.upper()}")
        st.markdown(f"**Table:** {st.session_state.table.name}")
        
        if st.session_state.loading_mode == 'chunked':
            st.markdown(f"**Estimated Rows:** ~{st.session_state.table.estimated_rows:,}")
            st.markdown(f"**Chunk Size:** {st.session_state.chunk_size:,}")
        else:
            st.markdown(f"**Rows:** {len(st.session_state.table.data):,}")
        
        st.markdown(f"**Columns:** {len(st.session_state.table.headers)}")
        st.markdown(f"**Operations:** {len(st.session_state.query_operations)}")
    
    st.divider()
    st.caption("Built by: Lance Dsilva, Rafayel Mirijanyan, Chelroy Limas")
    st.caption("DSCI 551 - Fall 2025")

# Cleanup temp files on session end
if st.session_state.get('temp_file_path') and not st.session_state.data_loaded:
    try:
        os.unlink(st.session_state.temp_file_path)
    except:
        pass
