"""
Query Engine for chaining SQL-like operations
Optimized for large files (up to 500MB) with true chunked processing
"""

class QueryEngine:
    def __init__(self, table):
        """
        Initialize QueryEngine with a table
        
        Args:
            table (Table): Table object to query
        """
        self.table = table
        self.operations = []
    
    def filter(self, condition):
        """Add filter operation to chain"""
        self.operations.append(('filter', condition))
        return self
    
    def select(self, columns):
        """Add select/projection operation to chain"""
        self.operations.append(('select', columns))
        return self
    
    def group_by(self, column):
        """Add group by operation to chain"""
        self.operations.append(('group_by', column))
        return self
    
    def aggregate(self, column, function):
        """Add aggregation operation to chain"""
        self.operations.append(('aggregate', (column, function)))
        return self
    
    def order_by(self, column, ascending=True):
        """Add order by operation to chain"""
        self.operations.append(('order_by', (column, ascending)))
        return self
    
    def limit(self, n):
        """Add limit operation to chain"""
        self.operations.append(('limit', n))
        return self
    
    def execute(self):
        """Execute all operations in the chain"""
        if self.table.chunked:
            return self._execute_chunked()
        else:
            return self._execute_normal()
    
    def _execute_normal(self):
        """Execute operations on full dataset"""
        result = self.table.data
        
        for op_type, op_param in self.operations:
            if op_type == 'filter':
                result = [row for row in result if op_param(row)]
            
            elif op_type == 'select':
                result = [{col: row.get(col) for col in op_param} for row in result]
            
            elif op_type == 'group_by':
                groups = {}
                for row in result:
                    key = row.get(op_param)
                    if key not in groups:
                        groups[key] = []
                    groups[key].append(row)
                result = groups
            
            elif op_type == 'aggregate':
                column, function = op_param
                if function == 'SUM':
                    result = sum(float(row.get(column, 0)) for row in result)
                elif function == 'AVG':
                    values = [float(row.get(column, 0)) for row in result]
                    result = sum(values) / len(values) if values else 0
                elif function == 'COUNT':
                    result = len(result)
                elif function == 'MIN':
                    result = min(float(row.get(column, 0)) for row in result)
                elif function == 'MAX':
                    result = max(float(row.get(column, 0)) for row in result)
            
            elif op_type == 'order_by':
                column, ascending = op_param
                result = sorted(result, key=lambda x: x.get(column), reverse=not ascending)
            
            elif op_type == 'limit':
                result = result[:op_param]
        
        return result
    
    def _execute_chunked(self):
        """
        Execute operations with TRUE chunked processing
        Processes each chunk independently and accumulates results
        """
        results = []
        limit_value = None
        has_order_by = False
        group_by_col = None
        groups = {}
        
        # Check for operations that need special handling
        for op_type, op_param in self.operations:
            if op_type == 'limit':
                limit_value = op_param
            elif op_type == 'order_by':
                has_order_by = True
            elif op_type == 'group_by':
                group_by_col = op_param
        
        # Process each chunk
        chunks_processed = 0
        for chunk in self.table.get_chunks():
            chunk_result = chunk
            chunks_processed += 1
            
            # Apply per-chunk operations
            for op_type, op_param in self.operations:
                if op_type == 'filter':
                    chunk_result = [row for row in chunk_result if op_param(row)]
                
                elif op_type == 'select':
                    chunk_result = [{col: row.get(col) for col in op_param} for row in chunk_result]
                
                elif op_type == 'group_by':
                    # Accumulate groups across chunks
                    for row in chunk_result:
                        key = row.get(op_param)
                        if key not in groups:
                            groups[key] = []
                        groups[key].append(row)
                    chunk_result = []  # Don't add to results yet
            
            results.extend(chunk_result)
            
            # Early termination for LIMIT
            if limit_value and len(results) >= limit_value and not has_order_by:
                results = results[:limit_value]
                break
            
            # Memory optimization: if we have way more than limit, trim
            if limit_value and len(results) > limit_value * 2:
                results = results[:limit_value * 2]
        
        # If we grouped, use groups as results
        if group_by_col:
            results = groups
        
        # Apply operations that need full dataset
        for op_type, op_param in self.operations:
            if op_type == 'order_by' and isinstance(results, list):
                column, ascending = op_param
                results = sorted(results, key=lambda x: x.get(column), reverse=not ascending)
            
            elif op_type == 'aggregate':
                column, function = op_param
                if isinstance(results, dict):
                    # Aggregate within groups
                    agg_results = {}
                    for key, group_rows in results.items():
                        if function == 'SUM':
                            agg_results[key] = sum(float(row.get(column, 0)) for row in group_rows)
                        elif function == 'AVG':
                            values = [float(row.get(column, 0)) for row in group_rows]
                            agg_results[key] = sum(values) / len(values) if values else 0
                        elif function == 'COUNT':
                            agg_results[key] = len(group_rows)
                        elif function == 'MIN':
                            agg_results[key] = min(float(row.get(column, 0)) for row in group_rows)
                        elif function == 'MAX':
                            agg_results[key] = max(float(row.get(column, 0)) for row in group_rows)
                    results = agg_results
                else:
                    # Aggregate all results
                    if function == 'SUM':
                        results = sum(float(row.get(column, 0)) for row in results)
                    elif function == 'AVG':
                        values = [float(row.get(column, 0)) for row in results]
                        results = sum(values) / len(values) if values else 0
                    elif function == 'COUNT':
                        results = len(results)
                    elif function == 'MIN':
                        results = min(float(row.get(column, 0)) for row in results)
                    elif function == 'MAX':
                        results = max(float(row.get(column, 0)) for row in results)
        
        # Apply final limit
        if limit_value and isinstance(results, list):
            results = results[:limit_value]
        
        return results
